# =======================================================================
# Platform Name            platys-platform
# Platform Stack:          trivadis/platys-modern-data-platform
# Platform Stack Version:  develop
# =======================================================================
networks:
  default:
    name: platys-platform
services:
#  ================================== Kafka ========================================== #
  kafka-1:
    image: confluentinc/cp-kafka:7.9.1
    container_name: kafka-1
    hostname: kafka-1
    labels:
      com.platys.name: kafka
      com.platys.description: Kafka Broker 1
    ports:
      - 9092:9092
      - 19092:19092
      - 29092:29092
      - 39092:39092
      - 9992:9992
      - 1234:1234
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_BROKER_RACK: rack1
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-1:49092,2@kafka-2:49093,3@kafka-3:49094
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 1
      CLUSTER_ID: y4vRIwfDT0SkZ65tD7Ey2A
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT,LOCAL:PLAINTEXT,DOCKERHOST:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: CONTROLLER://kafka-1:49092,BROKER://kafka-1:19092,LOCAL://kafka-1:39092,DOCKERHOST://kafka-1:29092,EXTERNAL://kafka-1:9092
      KAFKA_ADVERTISED_LISTENERS: BROKER://kafka-1:19092,LOCAL://localhost:39092,DOCKERHOST://${DOCKER_HOST_IP:-127.0.0.1}:29092,EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_MESSAGE_TIMESTAMP_TYPE: CreateTime
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_JMX_PORT: 9992
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9992
      KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test: kafka-topics --bootstrap-server kafka-1:19092 --list
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
  kafka-2:
    image: confluentinc/cp-kafka:7.9.1
    container_name: kafka-2
    hostname: kafka-2
    labels:
      com.platys.name: kafka
      com.platys.description: Kafka Broker 2
    ports:
      - 9093:9093
      - 19093:19093
      - 29093:29093
      - 39093:39093
      - 9993:9993
      - 1235:1234
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_BROKER_RACK: rack1
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-1:49092,2@kafka-2:49093,3@kafka-3:49094
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 2
      CLUSTER_ID: y4vRIwfDT0SkZ65tD7Ey2A
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT,LOCAL:PLAINTEXT,DOCKERHOST:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: CONTROLLER://kafka-2:49093,BROKER://kafka-2:19093,LOCAL://kafka-2:39093,DOCKERHOST://kafka-2:29093,EXTERNAL://kafka-2:9093
      KAFKA_ADVERTISED_LISTENERS: BROKER://kafka-2:19093,LOCAL://localhost:39093,DOCKERHOST://${DOCKER_HOST_IP:-127.0.0.1}:29093,EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_MESSAGE_TIMESTAMP_TYPE: CreateTime
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_JMX_PORT: 9993
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9993
      KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test: kafka-topics --bootstrap-server kafka-1:19092 --list
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
  kafka-3:
    image: confluentinc/cp-kafka:7.9.1
    container_name: kafka-3
    hostname: kafka-3
    labels:
      com.platys.name: kafka
      com.platys.description: Kafka Broker 3
    ports:
      - 9094:9094
      - 19094:19094
      - 29094:29094
      - 39094:39094
      - 9994:9994
      - 1236:1234
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_BROKER_RACK: rack1
      KAFKA_INTER_BROKER_LISTENER_NAME: BROKER
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-1:49092,2@kafka-2:49093,3@kafka-3:49094
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_NODE_ID: 3
      CLUSTER_ID: y4vRIwfDT0SkZ65tD7Ey2A
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,BROKER:PLAINTEXT,LOCAL:PLAINTEXT,DOCKERHOST:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_LISTENERS: CONTROLLER://kafka-3:49094,BROKER://kafka-3:19094,LOCAL://kafka-3:39094,DOCKERHOST://kafka-3:29094,EXTERNAL://kafka-3:9094
      KAFKA_ADVERTISED_LISTENERS: BROKER://kafka-3:19094,LOCAL://localhost:39094,DOCKERHOST://${DOCKER_HOST_IP:-127.0.0.1}:29094,EXTERNAL://${PUBLIC_IP:-127.0.0.1}:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_MESSAGE_TIMESTAMP_TYPE: CreateTime
      KAFKA_MIN_INSYNC_REPLICAS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'True'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'False'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_JMX_PORT: 9994
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=9994
      KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
      KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
      KAFKA_TOOLS_LOG4J_LOGLEVEL: INFO
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test: kafka-topics --bootstrap-server kafka-1:19092 --list
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
  #  ================================== Confluent Schema Registry ========================================== #
  schema-registry-1:
    image: confluentinc/cp-schema-registry:7.9.1
    hostname: schema-registry-1
    container_name: schema-registry-1
    labels:
      com.platys.name: schema-registry
      com.platys.description: Confluent Schema Registry
      com.platys.restapi.title: Schema Registry REST API
      com.platys.restapi.url: http://dataplatform:8081
    ports:
      - 8081:8081
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      SCHEMA_REGISTRY_GROUP_ID: schema-registry
      SCHEMA_REGISTRY_LEADER_ELIGIBILITY: 'True'
      SCHEMA_REGISTRY_MODE_MUTABILITY: 'True'
      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: backward
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 1
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS: GET,POST,PUT,OPTIONS
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: info
      SCHEMA_REGISTRY_DEBUG: 'False'
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: PLAINTEXT
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/opentelemetry/agents:/otel
    restart: unless-stopped
    healthcheck:
      start_period: 10s
      interval: 10s
      retries: 20
      test: curl --fail --silent --insecure http://schema-registry-1:8081/subjects --output /dev/null || exit 1
  #  ================================== Apache Kafka HQ (AKHQ) ========================================== #
  akhq:
    image: tchiotludo/akhq:latest
    container_name: akhq
    hostname: akhq
    labels:
      com.platys.name: akhq
      com.platys.description: Kafka GUI
      com.platys.webui.title: AKHQ UI
      com.platys.webui.url: http://dataplatform:28107
      com.platys.restapi.title: AKHQ API
      com.platys.restapi.url: http://dataplatform:28107/api
      com.platys.password.envvars: PLATYS_AKHQ_ADMIN_PASSWORD,PLATYS_AKHQ_READER_PASSWORD
    ports:
      - 28107:8080
      - 28320:28081
    environment:
      AKHQ_CONFIGURATION: |
        akhq:
          pagination.page-size: 25
          topic-data:
            size: 50
            poll-timeout: 1000
            kafka-max-message-length: 1000000
          ui-options:
            topic:
              default-view: HIDE_INTERNAL
              skip-consumer-groups: false
              skip-last-record: true
              show-all-consumer-groups: true
            topic-data:
              sort: OLDEST
          connections:
            docker-kafka-server:
              properties:
                bootstrap.servers: 'kafka-1:19092,kafka-2:19093,kafka-3:19094'
              schema-registry:
                url: "http://schema-registry-1:8081"
                type: "confluent"
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Apache Hive Server ========================================== #
  hive-server:
    image: trivadis/apache-hive:4.0.1-postgresql-metastore-s3
    container_name: hive-server
    hostname: hive-server
    labels:
      com.platys.name: hive
      com.platys.description: Hive Server
      com.platys.webui.title: Hive Server UI
      com.platys.webui.url: http://dataplatform:10002
    depends_on:
      hive-metastore:
        condition: service_healthy
    ports:
      - 10000:10000
      - 10001:10001
      - 10002:10002
    environment:
      SERVICE_PRECONDITION: hive-metastore:9083
      SERVICE_NAME: hiveserver2
      SERVICE_OPTS: -Dhive.metastore.uris=thrift://hive-metastore:9083
      IS_RESUME: 'true'
      HIVE_SITE_CONF_fs_defaultFS: s3a://admin-bucket
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:-admin}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      HIVE_SITE_CONF_hive_metastore_uris: thrift://hive-metastore:9083
      HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: jdbc:postgresql://hive-metastore-db/metastore
      HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: org.postgresql.Driver
      HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName: hive
      HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: hive
      HIVE_SITE_CONF_datanucleus_autoCreateSchema: false
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test: [CMD, nc, -z, hive-server, '10000']
      interval: 30s
      timeout: 10s
      retries: 10
  #  ================================== Apache Hive Metastore ========================================== #
  hive-metastore:
    image: trivadis/apache-hive:4.0.1-postgresql-metastore-s3
    container_name: hive-metastore
    hostname: hive-metastore
    labels:
      com.platys.name: hive-metastore
      com.platys.description: Hive Metastore
    depends_on:
      hive-metastore-db:
        condition: service_healthy
    ports:
      - 9083:9083
    environment:
      HIVE_VER: 4.0.1
      CORE_CONF_fs_defaultFS: file:///tmp
      HIVE_SITE_CONF_fs_defaultFS: file:///tmp
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: ${PLATYS_AWS_ACCESS_KEY:-admin}
      HIVE_SITE_CONF_fs_s3a_secret_key: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123}
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      HIVE_SITE_CONF_hive_metastore_uris: thrift://hive-metastore:9083
      HIVE_SITE_CONF_javax_jdo_option_ConnectionURL: jdbc:postgresql://hive-metastore-db/metastore_db
      HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName: org.postgresql.Driver
      HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName: hive
      HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: ${PLATYS_HMS_POSTGRESQL_PASSWORD:-abc123!}
      HIVE_SITE_CONF_datanucleus_autoCreateSchema: false
      HIVE_SITE_CONF_hive_metastore_event_db_notification_api_auth: false
      # necessary for Trino to be able to read from Avro
      HIVE_SITE_CONF_metastore_storage_schema_reader_impl: org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader
      SERVICE_PRECONDITION: hive-metastore-db:5432
      HIVE_SITE_CONF_hive_metastore_transactional_event_listeners: org.apache.hive.hcatalog.listener.KafkaNotificationListener,org.apache.hive.hcatalog.listener.DbNotificationListener
      HIVE_SITE_CONF_hive_metastore_event_db_listener_timetolive: 86400   # 1 day
      HIVE_SITE_CONF_hive_metastore_event_db_listener_clean_interval: 7200  # 2 hours
      DB_DRIVER: postgres
      SERVICE_NAME: metastore
      SERVICE_OPTS: -Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://hive-metastore-db/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=${PLATYS_HMS_POSTGRESQL_PASSWORD:-abc123!}
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/hive-metastore/hms-kafka-listener-0.1.0.jar:/opt/hive/lib/hms-kafka-listener-0.1.0.jar
    restart: unless-stopped
    healthcheck:
      test: [CMD, nc, -z, hive-metastore, '9083']
      interval: 30s
      timeout: 10s
      retries: 10
  hive-metastore-db:
    image: postgres
    container_name: hive-metastore-db
    hostname: hive-metastore-db
    labels:
      com.platys.name: hive-metastore
      com.platys.description: Hive Metastore DB
      com.platys.password.envvars: PLATYS_HMS_POSTGRESQL_PASSWORD
    ports:
      - 5442:5432
    environment:
      POSTGRES_DB: metastore_db
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: ${PLATYS_HMS_POSTGRESQL_PASSWORD:-abc123!}
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
    healthcheck:
      test: [CMD, psql, -U, hive, metastore_db]
  #  ================================== Jupyter ========================================== #
  jupyter:
    image: quay.io/jupyter/minimal-notebook:latest
    container_name: jupyter
    hostname: jupyter
    labels:
      com.platys.name: jupyter
      com.platys.description: Web-based interactive development environment for notebooks, code, and data
      com.platys.webui.title: Jupyter UI
      com.platys.webui.url: http://dataplatform:28888
      com.platys.password.envvars: PLATYS_JUPYTER_TOKEN,PLATYS_AWS_SECRET_ACCESS_KEY
    ports:
      - 28888:8888
    user: root
    extra_hosts:
      - host.docker.internal:host-gateway
    environment:
      JUPYTER_ENABLE_LAB: "'yes'"
      GRANT_SUDO: "'yes'"
      JUPYTER_TOKEN: ${PLATYS_JUPYTER_TOKEN:-abc123!}
      DOCKER_STACKS_JUPYTER_CMD: lab
      MAVEN_DOWNLOAD_JARS: com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-aws:3.3.4,com.google.guava:guava:27.1-jre
      # remove some JARS if they are conflicting with the ones installed above
      REMOVE_JARS: guava-14.0.1.jar
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: ${PLATYS_AWS_ACCESS_KEY:-admin}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123}
      AWS_ENDPOINT: http://minio-1:9000
      AWS_REGION: us-east-1
      AWS_DEFAULT_REGION: us-east-1
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/jupyter/on-startup-jupyter/:/usr/local/bin/start-notebook.d/
      - ./init/jupyter/on-startup-jupyter-finished/:/usr/local/bin/before-notebook.d/
      - ./init/jupyter/on-startup-notebook-kernel:/home/jovyan/.ipython/profile_default/startup/
      - ./scripts/docker/maven-download.sh:/maven-download.sh
    command:
      # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
      - bash
      - -c
      - |
        conda create -y --name py312 python=3.12.8
        source /opt/conda/etc/profile.d/conda.sh
        conda activate py312
        pip install ipykernel    
        python -m ipykernel install --user --name py312 --display-name "Python 3.12.8 (ipykernel)"
        start-notebook.sh
    restart: unless-stopped
  #  ================================== Trino ========================================== #
  trino-1:
    image: trinodb/trino:476
    hostname: trino-1
    container_name: trino-1
    labels:
      com.platys.name: trino
      com.platys.description: SQL Virtualization Engine
      com.platys.webui.title: Trino UI
      com.platys.webui.url: http://dataplatform:28082/ui/preview
      com.platys.password.envvars: PLATYS_GSHEET_CREDENTIALS_KEY
    ports:
      - 28082:8080
      - 28087:8443
    environment:
      COORDINATOR_HOST: trino-1
      CATALOG_MANAGEMENT: static
      CATALOG_CONFIG_DIR: /catalog
      #CATALOG_STORE: file
      S3_ENDPOINT: http://minio-1:9000
      S3_REGION: us-east-1
      S3_AWS_ACCESS_KEY: ${PLATYS_AWS_ACCESS_KEY:-admin}
      S3_AWS_SECRET_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123}
      S3_PATH_STYLE_ACCESS: 'true'
      HIVE_STORAGE_FORMAT: ORC
      HIVE_COMPRESSION_CODEC: GZIP
      HIVE_VIEWS_ENABLED: 'false'
      HIVE_RUN_AS_INVOKER: 'false'
      HIVE_LEGACY_TRANSLATION: 'false'
      NESSIE_CATALOG_WAREHOUSE_DIR: s3a://admin-bucket/nessie/warehouse
      ICEBERG_REST_CATALOG_URI: http://iceberg-rest-catalog:8181
      ICEBERG_REST_CATALOG_WAREHOUSE: s3a://admin-bucket/iceberg/warehouse
      HIVE_METASTORE_DB_USER: hive
      HIVE_METASTORE_DB_PASSWORD: ${PLATYS_HMS_POSTGRESQL_PASSWORD:-abc123!}
      EVENT_LISTENER_CONFIG_FILES: ''
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/trino/single/config.properties:/etc/trino/config.properties
      - ./conf/trino/single/node.properties:/etc/trino/node.properties
      - ./conf/trino/single/log-debug.properties:/etc/trino/log.properties
      - ./conf/trino/catalog/minio.properties:/catalog/minio.properties
      - ./conf/trino/catalog/hive_metastore_db.properties:/catalog/hive_metastore_db.properties
      - ./custom-conf/trino/security:/etc/trino/security
    restart: unless-stopped
    healthcheck:
      test: [CMD-SHELL, trino --version]
      interval: 5s
      timeout: 5s
      retries: 5
  trino-cli:
    image: trivadis/trino-cli:latest
    hostname: trino-cli
    container_name: trino-cli
    labels:
      com.platys.name: trino
      com.platys.description: Trino CLI
    volumes:
      - ./data-transfer:/data-transfer
    tty: true
    restart: unless-stopped
  #  ================================== Minio ========================================== #
  minio-1:
    image: minio/minio:RELEASE.2025-05-24T17-08-30Z
    container_name: minio-1
    hostname: minio-1
    labels:
      com.platys.name: minio
      com.platys.description: Software-defined Object Storage
      com.platys.webui.title: MinIO UI
      com.platys.webui.url: http://dataplatform:9010
      com.platys.password.envvars: PLATYS_AWS_SECRET_ACCESS_KEY
    ports:
      # S3 API Port
      - 9000:9000
      # UI Port
      - 9010:9010
    environment:
      MINIO_ROOT_USER: ${PLATYS_AWS_ACCESS_KEY:-admin}
      MINIO_ROOT_PASSWORD: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123}
      # remove region due to problems with RisingWave
      #MINIO_REGION_NAME: us-east-1
      #MINIO_REGION: us-east-1
      MINIO_DOMAIN: minio
      MINIO_SERVER_URL: http://${PUBLIC_IP}:9000
      MINIO_COMPRESSION_ENABLE: off
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: http://prometheus-1:9090
      MINIO_AUDIT_KAFKA_ENABLE: on
      MINIO_AUDIT_KAFKA_BROKERS: kafka-1:19092,kafka-2:19093,kafka-3:19094
      MINIO_AUDIT_KAFKA_TOPIC: minio-audit-log
    volumes:
      - ./data-transfer:/data-transfer
      - ./container-volume/minio:/data/
    user: '1000'
    command: server /data --console-address ":9010"
    restart: unless-stopped
    healthcheck:
      test: [CMD, curl, -f, http://minio-1:9000/minio/health/live]
      interval: 15s
      timeout: 20s
      retries: 3
  #  ================================== Minio MC ========================================== #
  minio-mc:
    image: minio/mc:latest
    container_name: minio-mc
    hostname: minio-mc
    labels:
      com.platys.name: minio
      com.platys.description: MinIO Console
    environment:
      # these two env variables are also needed for the s3-credentials.properties file gen to work! 
      AWS_ACCESS_KEY: ${PLATYS_AWS_ACCESS_KEY:-admin}
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123}
      MC_HOST_minio-1: http://${PLATYS_AWS_ACCESS_KEY:-admin}:${PLATYS_AWS_SECRET_ACCESS_KEY:-abc123abc123}@minio-1:9000
    volumes:
      - ./data-transfer:/data-transfer
      - ./scripts/docker/wait-for-it.sh:/usr/src/app/wait-for-it.sh
      - ./security/aws/credentials:/tmp/credentials.templ
      - aws-credentials-vol:/tmp/.aws:RO
    entrypoint:
      - /bin/sh
      - -c
      - |
        /usr/src/app/wait-for-it.sh -t 180 minio-1:9000
        mkdir -p /tmp/.aws
        eval "echo \"$$(cat /tmp/credentials.templ)\"" >> /tmp/.aws/credentials
        mc mb --ignore-existing minio-1/admin-bucket
              for bucket in $$(tr ',' '\n' <<< "")
        do
          mc mb --ignore-existing minio-1/$$bucket
        done
        #
        while [ 1 -eq 1 ];do sleep 60;done
    restart: unless-stopped
  #  ================================== Adminio UI ========================================== #
  adminio-ui:
    image: rzrbld/adminio-ui:latest
    container_name: adminio-ui
    hostname: adminio-ui
    labels:
      com.platys.name: adminio
      com.platys.description: MinIO Admin UI
      com.platys.webui.title: Adminio UI
      com.platys.webui.url: http://dataplatform:28190
    ports:
      - 28190:80
    environment:
      API_BASE_URL: http://${PUBLIC_IP}:28191
      ADMINIO_MULTI_BACKEND: 'false'
      ADMINIO_BACKENDS: '[{"name":"myminio","url":"http://${PUBLIC_IP}:28191"}]'
      NGX_ROOT_PATH: /
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  adminio-api:
    image: rzrbld/adminio-api:latest
    container_name: adminio-api
    hostname: adminio-api
    labels:
      com.platys.name: adminio
      com.platys.description: MinIO Admin UI
      com.platys.restapi.title: Adminio API
      com.platys.restapi.url: http://dataplatform:28191
    ports:
      - 28191:8080
    environment:
      MINIO_ACCESS: admin
      MINIO_SECRET: abc123abc123
      MINIO_HOST_PORT: minio-1:9000
      #MINIO_SSE_MASTER_KEY: 1:da2f4cfa32bed76507dcd44b42872328a8e14f25cd2a1ec0fb85d299a192a447
      ADMINIO_HOST_PORT: 0.0.0.0:8080
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== markdown-viewer ========================================== #
  markdown-viewer:
    image: dannyben/madness:latest
    container_name: markdown-viewer
    hostname: markdown-viewer
    labels:
      com.platys.name: markdown-viewer
      com.platys.description: Platys Platform homepage viewer
      com.platys.webui.title: Markdown Viewer UI
      com.platys.webui.url: http://dataplatform:80
    ports:
      - 80:3000
    volumes:
      - ./artefacts:/docs
      - ./conf/markdown-viewer/markdown-madness.yml:/docs/.madness.yml
      - ./data-transfer:/data-transfer
    command: server
    restart: unless-stopped
    healthcheck:
      test: [CMD-SHELL, curl -f http://markdown-viewer:3000 || exit 1]
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 1m
  markdown-renderer:
    image: trivadis/jinja2-renderer:0.8.1
    container_name: markdown-renderer
    hostname: markdown-renderer
    labels:
      com.platys.name: markdown-renderer
      com.platys.description: Platys Platform homepage rendering
    environment:
      USE_PUBLIC_IP: 'True'
      PUBLIC_IP: ${PUBLIC_IP}
      DOCKER_HOST_IP: ${DOCKER_HOST_IP}
      DATAPLATFORM_HOME: ${DATAPLATFORM_HOME}
      PLATYS_PLATFORM_NAME: platys-platform
      PLATYS_PLATFORM_STACK: trivadis/platys-modern-data-platform
      PLATYS_PLATFORM_STACK_VERSION: develop
      PLATYS_COPY_COOKBOOK_DATA: 'True'
      PLATYS_DATACENTER:
      SERVICE_LIST_VERSION: 2
    volumes:
      - ./artefacts/templates:/templates
      - ./artefacts/templates:/scripts
      - .:/variables
      - ./artefacts:/output
      - ./data-transfer:/data-transfer
    init: true
  #  ================================== data-provisioning ========================================== #
  data-provisioning:
    image: trivadis/platys-modern-data-platform-data:latest
    container_name: data-provisioning
    hostname: data-provisioning
    labels:
      com.platys.name: data-provisioning
      com.platys.description: Provisioning sample data
    environment:
      DUMMY: make-it-valid
    volumes:
      - ./data-transfer:/data-transfer
    init: true
volumes:
  data-transfer-vol:
    name: data_transfer_vol
  aws-credentials-vol:
